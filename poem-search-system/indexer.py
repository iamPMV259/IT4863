import json
import logging
import re
import sys
import time

from elasticsearch import Elasticsearch, helpers
from sentence_transformers import SentenceTransformer

# --- C·∫§U H√åNH LOGGING ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)

# --- C·∫§U H√åNH K·∫æT N·ªêI ---
# D√πng 127.0.0.1 ƒë·ªÉ kh·ªõp v·ªõi k·∫øt qu·∫£ curl th√†nh c√¥ng c·ªßa b·∫°n
ES_HOST = "http://127.0.0.1:9200"
INDEX_NAME = "poems_hust_project"

# TƒÉng timeout l√™n 60s ƒë·ªÉ tr√°nh l·ªói khi m√°y lag
es = Elasticsearch(ES_HOST, request_timeout=60)

# Load Model AI
# L∆ØU √ù: L·∫ßn ch·∫°y ƒë·∫ßu ti√™n s·∫Ω m·∫•t kho·∫£ng 5-10 ph√∫t ƒë·ªÉ t·∫£i file 540MB.
# Vui l√≤ng KH√îNG t·∫Øt ngang khi th·∫•y thanh % ƒëang ch·∫°y.
logging.info("‚è≥ ƒêang t·∫£i model AI (keepitreal/vietnamese-sbert)...")
model = SentenceTransformer('keepitreal/vietnamese-sbert')


def clean_poem_content(raw_text):
    """
    H√†m l√†m s·∫°ch d·ªØ li·ªáu:
    1. Lo·∫°i b·ªè c√°c d√≤ng ch·ª©a ch·ªØ H√°n/N√¥m.
    2. Lo·∫°i b·ªè c√°c d√≤ng tr√πng l·∫∑p.
    3. Gi·ªØ l·∫°i ti·∫øng Vi·ªát s·∫°ch.
    """
    if not raw_text:
        return ""

    lines = raw_text.split('\n')
    clean_lines = []
    seen_lines = set()

    for line in lines:
        line = line.strip()
        
        # 1. B·ªè d√≤ng r·ªóng ho·∫∑c qu√° ng·∫Øn
        if not line or len(line) < 2:
            continue
            
        # 2. B·ªè d√≤ng ch·ª©a k√Ω t·ª± H√°n/N√¥m (Unicode range \u4e00-\u9fff)
        if re.search('[\u4e00-\u9fff]', line):
            continue
            
        # 3. B·ªè d√≤ng tr√πng l·∫∑p (check kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng)
        line_lower = line.lower()
        if line_lower in seen_lines:
            continue
        
        seen_lines.add(line_lower)
        clean_lines.append(line)
    
    # Gh√©p l·∫°i th√†nh ƒëo·∫°n vƒÉn
    return "\n".join(clean_lines)


def create_index():
    """T·∫°o Index v√† Mapping trong Elasticsearch"""
    
    # --- FIX QUAN TR·ªåNG: D√πng info() thay v√¨ ping() ƒë·ªÉ tr√°nh l·ªói 400 ---
    try:
        server_info = es.info()
        logging.info(f"üîó K·∫øt n·ªëi th√†nh c√¥ng t·ªõi Elasticsearch v{server_info['version']['number']}")
    except Exception as e:
        logging.error(f"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn {ES_HOST}. L·ªói chi ti·∫øt: {e}")
        logging.error("üëâ H√£y ki·ªÉm tra: docker ps (xem container ch·∫°y ch∆∞a)")
        sys.exit(1)

    # X√≥a index c≈© n·∫øu t·ªìn t·∫°i (ignore l·ªói 400/404)
    if es.indices.exists(index=INDEX_NAME):
        es.indices.delete(index=INDEX_NAME, ignore=[400, 404])
        logging.info(f"üóëÔ∏è ƒê√£ x√≥a index c≈©: {INDEX_NAME}")
    
    # ƒê·ªãnh nghƒ©a c·∫•u tr√∫c d·ªØ li·ªáu
    settings = {
        "mappings": {
            "properties": {
                # --- Field cho Full-text Search (BM25) ---
                "poem_title": {"type": "text", "analyzer": "standard"},
                "author": {"type": "keyword"},
                "poem_content_text": {"type": "text", "analyzer": "standard"},
                "the_tho": {"type": "keyword"},
                "thoi_ky": {"type": "keyword"},
                
                # --- Field cho Vector Search (Semantic) ---
                "poem_vector": {
                    "type": "dense_vector",
                    "dims": 768,           # K√≠ch th∆∞·ªõc vector c·ªßa model sbert
                    "index": True,
                    "similarity": "cosine" # D√πng cosine similarity ƒë·ªÉ so s√°nh √Ω nghƒ©a
                }
            }
        }
    }
    
    es.indices.create(index=INDEX_NAME, body=settings)
    logging.info(f"‚úÖ ƒê√£ t·∫°o m·ªõi Index: {INDEX_NAME}")


def generate_docs():
    """ƒê·ªçc file JSON, l√†m s·∫°ch v√† vector h√≥a"""
    try:
        with open('poems.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        logging.error("‚ùå Kh√¥ng t√¨m th·∫•y file 'poems.json'. H√£y ch·∫Øc ch·∫Øn file data ƒëang ·ªü c√πng th∆∞ m·ª•c.")
        sys.exit(1)
    
    logging.info(f"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(data)} b√†i th∆° ƒë·∫ßu v√†o...")
    
    count_valid = 0
    count_skipped = 0
    
    for poem in data:
        # 1. L√†m s·∫°ch n·ªôi dung
        raw_content = poem.get('poem_content_text', '')
        cleaned_content = clean_poem_content(raw_content)
        
        # N·∫øu sau khi l·ªçc m√† n·ªôi dung qu√° ng·∫Øn (< 20 k√Ω t·ª±) th√¨ b·ªè qua
        if len(cleaned_content) < 20:
            count_skipped += 1
            continue

        # 2. T·∫°o Vector (Embedding)
        # K·∫øt h·ª£p Title + Content ƒë·ªÉ AI hi·ªÉu ng·ªØ c·∫£nh ƒë·∫ßy ƒë·ªß
        text_to_vector = f"{poem['poem_title']} {cleaned_content}"
        
        # Encode (C·∫Øt ng·∫Øn text xu·ªëng 512 t·ª´ ƒë·ªÉ tr√°nh qu√° t·∫£i model)
        vector = model.encode(text_to_vector[:1000]).tolist()
        
        count_valid += 1
        
        # 3. Tr·∫£ v·ªÅ document chu·∫©n ES
        yield {
            "_index": INDEX_NAME,
            "_source": {
                "poem_title": poem['poem_title'],
                "author": poem['author'],
                "poem_content_text": cleaned_content, # L∆∞u b·∫£n s·∫°ch
                "the_tho": poem.get('the_tho', ''),
                "thoi_ky": poem.get('thoi_ky', ''),
                "url": poem.get('url', ''),
                "poem_vector": vector
            }
        }
    
    logging.info(f"‚ö†Ô∏è ƒê√£ b·ªè qua {count_skipped} b√†i (r√°c/H√°n/N√¥m).")
    logging.info(f"üì¶ ƒêang ƒë·∫©y {count_valid} b√†i th∆° s·∫°ch v√†o Elasticsearch...")


def main():
    create_index()
    # Bulk index (ƒë·∫©y h√†ng lo·∫°t) ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô
    try:
        start_time = time.time()
        success, failed = helpers.bulk(es, generate_docs(), stats_only=True)
        duration = time.time() - start_time
        logging.info(f"üéâ HO√ÄN T·∫§T! ƒê√£ index {success} b√†i th∆° trong {duration:.2f}s.")
        if failed > 0:
            logging.warning(f"‚ö†Ô∏è C√≥ {failed} b√†i b·ªã l·ªói khi index.")
    except Exception as e:
        logging.error(f"‚ùå L·ªói trong qu√° tr√¨nh Bulk Index: {e}")

if __name__ == "__main__":
    main()