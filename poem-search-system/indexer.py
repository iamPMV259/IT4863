import json
import logging

from elasticsearch import Elasticsearch, helpers
from sentence_transformers import SentenceTransformer

# Setup Log cho d·ªÖ nh√¨n
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

# 1. K·∫øt n·ªëi Elasticsearch
es = Elasticsearch("http://localhost:9200")
INDEX_NAME = "poems_hust_project"

# 2. Load Model AI (Vietnamese SBERT)
# Model n√†y s·∫Ω bi·∫øn vƒÉn b·∫£n th√†nh vector 768 chi·ªÅu
logging.info("‚è≥ ƒêang t·∫£i model AI (l·∫ßn ƒë·∫ßu s·∫Ω h∆°i l√¢u)...")
model = SentenceTransformer('keepitreal/vietnamese-sbert')

def create_index():
    """T·∫°o c·∫•u tr√∫c b·∫£ng (Index Mapping)"""
    if es.indices.exists(index=INDEX_NAME):
        es.indices.delete(index=INDEX_NAME)
    
    settings = {
        "mappings": {
            "properties": {
                # --- Nh√≥m field cho BM25 & Hi·ªÉn th·ªã ---
                "poem_title": {"type": "text", "analyzer": "standard"},
                "author": {"type": "keyword"}, # Keyword ƒë·ªÉ filter ch√≠nh x√°c
                "poem_content_text": {"type": "text", "analyzer": "standard"},
                "the_tho": {"type": "keyword"},
                "thoi_ky": {"type": "keyword"},
                "url": {"type": "keyword"},
                
                # --- Nh√≥m field cho Semantic Search (Ch∆∞∆°ng 4) ---
                "poem_vector": {
                    "type": "dense_vector",
                    "dims": 768, # K√≠ch th∆∞·ªõc vector c·ªßa model sbert
                    "index": True,
                    "similarity": "cosine" # D√πng cosine similarity ƒë·ªÉ ƒëo kho·∫£ng c√°ch
                }
            }
        }
    }
    es.indices.create(index=INDEX_NAME, body=settings)
    logging.info(f"‚úÖ ƒê√£ t·∫°o Index: {INDEX_NAME}")

def generate_docs():
    """ƒê·ªçc JSON v√† t·∫°o generator ƒë·ªÉ ƒë·∫©y v√†o ES"""
    with open('poems.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    logging.info(f"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(data)} b√†i th∆°...")
    
    for poem in data:
        # K·∫øt h·ª£p Ti√™u ƒë·ªÅ + N·ªôi dung ƒë·ªÉ AI hi·ªÉu ng·ªØ c·∫£nh t·ªët h∆°n
        combined_text = f"{poem['poem_title']} {poem['poem_content_text']}"
        
        # T·∫°o Vector (Embedding)
        vector = model.encode(combined_text).tolist()
        
        # T·∫°o document ƒë·ªÉ ƒë·∫©y l√™n ES
        yield {
            "_index": INDEX_NAME,
            "_source": {
                "poem_title": poem['poem_title'],
                "author": poem['author'],
                "poem_content_text": poem['poem_content_text'],
                "the_tho": poem.get('the_tho', ''),
                "thoi_ky": poem.get('thoi_ky', ''),
                "url": poem.get('url', ''),
                "poem_vector": vector # Vector n·∫±m ·ªü ƒë√¢y
            }
        }

def main():
    create_index()
    # S·ª≠ d·ª•ng bulk helper ƒë·ªÉ ƒë·∫©y d·ªØ li·ªáu nhanh h∆°n
    success, _ = helpers.bulk(es, generate_docs())
    logging.info(f"üéâ Ho√†n t·∫•t! ƒê√£ index th√†nh c√¥ng {success} b√†i th∆°.")

if __name__ == "__main__":
    main()